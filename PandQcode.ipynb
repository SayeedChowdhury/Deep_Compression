{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying Conv1 layer of CIFAR10 Network using Pruning, Quantization and Huffman Encoding methods of the Deep Compression paper\n",
    "\n",
    "## Load the pretrained model, solvers, and look at the shape of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sbin/ldconfig.real: /usr/local/cuda/lib64/libcudnn.so.5 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so.5 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/lib/nvidia-375/libEGL.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/lib32/nvidia-375/libEGL.so.1 is not a symbolic link\r\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('conv1', (32, 3, 5, 5)),\n",
       " ('conv2', (32, 32, 5, 5)),\n",
       " ('conv3', (64, 32, 5, 5)),\n",
       " ('ip1', (10, 1024))]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo ldconfig /usr/local/cuda/lib64\n",
    "\n",
    "\n",
    "from pylab import *\n",
    "import os\n",
    "%matplotlib inline\n",
    "caffe_root = '/home/super/ISHA/Deep_Learning/Caffe/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    " \n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    " \n",
    "GPU_ID = 1 # Switch between 0 and 1 depending on the GPU you want to use.\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(1)\n",
    "os.chdir(caffe_root)\n",
    " \n",
    "solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "solver = caffe.SGDSolver('/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/cifar10_full_solver.prototxt')\n",
    "solver.net.copy_from('/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/reference.caffemodel.h5')\n",
    " \n",
    " \n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the distribution of the weights. Sort the weights by the absolute magnitude, with the most important weight being the one with the highest magnitude. Notice that most weights are concentrated near the origin, and the distribution looks like a Guassian centred at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('stats for layer', 'conv1')\n",
      "('max, min and mean value  of original data are ', 0.38542399, -0.36021158, -0.00019509663)\n",
      "('max, min and mean value  of absolute value of original data  are ', 0.38542399, 5.1449009e-05, 0.067123264)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAFkCAYAAACn/timAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X+U3XV95/HnCxAQbAZO0xBtTWtrwdR6rBnFUJWujYIW\n6mrtto7miLJ2rUXqmdYVf8BCYdsqbg1FqPWo1Qo6LmJb64JEwa5FpKiEKi0DWys6KCY6EAYaDEjy\n2T++38Gby2SSz507c2eS5+Oceyb38/18v9/POzPJfc3n+yulFCRJkvbWAYMegCRJWloMD5IkqYrh\nQZIkVTE8SJKkKoYHSZJUxfAgSZKqGB4kSVIVw4MkSapieJAkSVUMD5IkqUp1eEjymCQXJPlmkvuT\nfCHJ07v6nJvkznb5Z5M8sWv5kUk+kmQqydYk709y+FyLkSRJ86+XmYcPAOuAVwC/CHwWuDrJYwGS\nnAG8HngtcCywDdiY5OCObXwUWN1u5yTgeOC9PdYgSZIWUGoejJXkUOA+4NdLKVd1tH8FuLKU8j+S\n3Am8s5SyoV22DNgCnFJKuSzJauBfgeFSyk1tnxOBK4CfKqVs7lNtkiRpHtTOPBwEHAg80NX+A+DZ\nSZ4ArASumV5QSrkXuAE4rm1aC2ydDg6tq4ECPLNyPJIkaYEdVNO5lPIfSa4HzkpyK82MwstpgsG/\n0QSH0rZ32tIuo/36va7t7khyd0efXST5ceBE4JvA9poxS5K0nzsU+BlgYynlrn5ssCo8tNYDfwV8\nB3gI2ERzDsOaWdYJTaiYzWx9TgQ+UjdMSZLU4RU0n9dzVh0eSim3A89N8mhgWSllS5KPAbcDm2lC\nwFHsOvuwApg+TLG5ff+wJAcCR/LIGYtp3wS49NJLWb16de2Ql5TR0VE2bNgw6GHMO+vct+wvdcL+\nU6t17jvGx8dZv349tJ+l/dDLzAMApZQfAD9IciTNzMAbSym3J9lMcxXF1+DhEyafCVzcrno9cESS\np3Wc97COJnTcsJvdbQdYvXo1a9bMNsGx9A0NDe3zNYJ17mv2lzph/6nVOvdJfTvsXx0ekpxA80F/\nG/DzwPnAOPChtssFwJlJvk6Tcs4Dvg18EqCUcmuSjcD7krwOOBh4NzDmlRaSJC1+vcw8DAF/Cvwk\ncDdwOXBmKWUHQCnl/CSH0dy34QjgWuCFpZQHO7bxcuAimqssdrbbeEOvRUiSpIXTyzkPHwc+voc+\n5wDnzLL8HpoTLyVJ0hLjsy0WmZGRkUEPYUFY575lf6kT9p9arVOzqbrD5KAkWQPceOONN+5PJ7ZI\nkjRnmzZtYnh4GJo7O2/qxzadeZAkSVUMD5IkqYrhQZIkVTE8SJKkKoYHSZJUxfAgSZKqGB4kSVIV\nw4MkSapieJAkSVUMD5IkqYrhQZIkVTE8SJKkKoYHSZJU5aBBD0BSvYmJCSYnJwc9jL22fPlyVq1a\nNehhSOoTw4O0xExMTHDMMavZvv3+QQ9lrx166GHcdtu4AULaRxgepCVmcnKyDQ6XAqsHPZy9MM72\n7euZnJw0PEj7CMODtGStBtYMehCS9kOeMClJkqoYHiRJUhXDgyRJqmJ4kCRJVarCQ5IDkpyX5BtJ\n7k/y9SRnztDv3CR3tn0+m+SJXcuPTPKRJFNJtiZ5f5LD51qMJEmaf7UzD28GXgv8HvAk4E3Am5K8\nfrpDkjOA17f9jgW2ARuTHNyxnY/SnCq+DjgJOB54b481SJKkBVR7qeZxwCdLKVe17yeSvJwmJEx7\nA3BeKeVTAEleCWwBXgxclmQ1cCIwXEq5qe1zOnBFkjeWUjb3Xo4kSZpvtTMPXwTWJfl5gCRPBZ4F\nXNm+fwKwErhmeoVSyr3ADTTBA2AtsHU6OLSuBgrwzB5qkCRJC6h25uHtwDLg1iQ7aMLH20opH2uX\nr6QJAVu61tvSLpvu873OhaWUHUnu7ugjSZIWqdrw8NvAy4GXAbcAvwT8eZI7SymXzLJeaELFbPam\njyRJGrDa8HA+8CellI+37/81yc8AbwEuATbThICj2HX2YQUwfZhic/v+YUkOBI7kkTMWuxgdHWVo\naGiXtpGREUZGRirLkCRp3zM2NsbY2NgubVNTU33fT214OIxHzg7spD13opRye5LNNFdRfA0gyTKa\ncxkubvtfDxyR5Gkd5z2sowkdN8y28w0bNrBmjffylyRpJjP9Qr1p0yaGh4f7up/a8PAp4G1J7gD+\nleapPKPA+zv6XACcmeTrwDeB84BvA58EKKXcmmQj8L4krwMOBt4NjHmlhSRJi19teHg9TRi4mObQ\nw53Ae9o2AEop5yc5jOa+DUcA1wIvLKU82LGdlwMX0VxlsRO4nOYST0mStMhVhYdSyjbgD9rXbP3O\nAc6ZZfk9wPqafUuSpMXBZ1tIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSpiuFB\nkiRVMTxIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJ\nklTF8CBJkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVqQoPSW5PsnOG17vb5YckuTjJZJL7\nklyeZEXXNh6f5Iok25JsTnJ+EkOMJElLRO2H9tOBlR2v5wMFuKxdfgFwEvBS4HjgccAnplduQ8KV\nwEHAWuAU4FXAub0WIEmSFtZBNZ1LKXd1vk/y68C/l1KuTbIMOBV4WSnl8+3yVwPjSY4tpXwJOBF4\nEvDcUsokcHOSs4C3JzmnlPJQH2qSJEnzqOfDBUkeBbwC+EDb9HSaMHLNdJ9Sym3ABHBc27QWuLkN\nDtM2AkPAk3sdiyRJWjhVMw9dXkLzof/X7fujgAdLKfd29dtCc4iD9uuWGZZPL/vqHMYjaREbHx8f\n9BD2yvLly1m1atWghyEtanMJD6cCny6lbN5Dv9CcF7Ene+wzOjrK0NDQLm0jIyOMjIzsxeYlDcZ3\ngQNYv379oAeyVw499DBuu23cAKElaWxsjLGxsV3apqam+r6fnsJDklXA84AXdzRvBg5Osqxr9mEF\nP5pd2Aw8o2tzR7Vfu2ckHmHDhg2sWbOmlyFLGph7gJ3ApcDqAY9lT8bZvn09k5OThgctSTP9Qr1p\n0yaGh4f7up9eZx5Opfmwv7Kj7UbgIWAd8LcASY4GVgFfbPtcD7w1yfKO8x5OAKaAW3oci6QlYTVg\n+Jf2BdXhIUloLq/8UCll53R7KeXeJB8A3pVkK3AfcCFwXSnly223z9CEhEuSnAE8FjgPuKiU8sM5\nVSJJkhZELzMPzwMeD3xwhmWjwA7gcuAQ4CrgtOmFpZSdSU4G3kMzG7EN+BBwdg/jkCRJA1AdHkop\nnwUO3M2yB4DT29fu1r8DOLl2v5IkaXHwttCSJKmK4UGSJFUxPEiSpCqGB0mSVMXwIEmSqhgeJElS\nFcODJEmqYniQJElVDA+SJKmK4UGSJFUxPEiSpCqGB0mSVMXwIEmSqhgeJElSFcODJEmqYniQJElV\nDA+SJKmK4UGSJFUxPEiSpCqGB0mSVMXwIEmSqhgeJElSFcODJEmqUh0ekjwuySVJJpPcn+SrSdZ0\n9Tk3yZ3t8s8meWLX8iOTfCTJVJKtSd6f5PC5FiNJkuZfVXhIcgRwHfAAcCKwGvhDYGtHnzOA1wOv\nBY4FtgEbkxzcsamPtuuuA04Cjgfe23MVkiRpwRxU2f/NwEQp5TUdbd/q6vMG4LxSyqcAkrwS2AK8\nGLgsyWqa4DFcSrmp7XM6cEWSN5ZSNvdQhyRJWiC1hy1+HfhKksuSbEmyKcnDQSLJE4CVwDXTbaWU\ne4EbgOPaprXA1ung0LoaKMAze6hBkiQtoNrw8LPA64DbgBOAvwQuTLK+Xb6SJgRs6VpvS7tsus/3\nOheWUnYAd3f0kSRJi1TtYYsDgC+VUs5q3381yZNpAsWls6wXmlAxm73pI0mSBqw2PHwXGO9qGwd+\no/3zZpoQcBS7zj6sAG7q6LOicwNJDgSO5JEzFrsYHR1laGhol7aRkRFGRkb2vgJJkvZRY2NjjI2N\n7dI2NTXV9/3UhofrgGO62o6hPWmylHJ7ks00V1F8DSDJMppzGS5u+18PHJHkaR3nPayjCR03zLbz\nDRs2sGbNmtm6SJK035rpF+pNmzYxPDzc1/3UhocNwHVJ3gJcRhMKXgP8TkefC4Azk3wd+CZwHvBt\n4JMApZRbk2wE3pfkdcDBwLuBMa+0kCRp8asKD6WUryR5CfB24CzgduANpZSPdfQ5P8lhNPdtOAK4\nFnhhKeXBjk29HLiI5iqLncDlNJd4SpKkRa525oFSypXAlXvocw5wzizL7wHW7265JElavHy2hSRJ\nqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSp\niuFBkiRVMTxIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQq\nhgdJklTF8CBJkqoYHiRJUpWq8JDk7CQ7u163dCw/JMnFSSaT3Jfk8iQrurbx+CRXJNmWZHOS85MY\nYiRJWiIO6mGdfwHWAWnfP9Sx7ALghcBLgXuBi4FPAM8BaEPClcCdwFrgccAlwIPAmT2MRZIkLbBe\nwsNDpZTvdzcmWQacCryslPL5tu3VwHiSY0spXwJOBJ4EPLeUMgncnOQs4O1JzimlPNS9XUmStLj0\ncrjg55N8J8m/J7k0yePb9mGaMHLNdMdSym3ABHBc27QWuLkNDtM2AkPAk3sYiyRJWmC14eGfgFfR\nzCD8LvAE4B+THA6sBB4spdzbtc6Wdhnt1y0zLKejjyRJWsSqDluUUjZ2vP2XJF8CvgX8FrB9N6sF\nKHuz+T11GB0dZWhoaJe2kZERRkZG9mLzkiTt28bGxhgbG9ulbWpqqu/76eWch4eVUqaS/D/gicDV\nwMFJlnXNPqzgR7MLm4FndG3mqPZr94zEI2zYsIE1a9bMZciSJO2zZvqFetOmTQwPD/d1P3O6RDLJ\nY4Cfo7l64kaaKy/WdSw/GlgFfLFtuh54SpLlHZs5AZgCbkGSJC16VTMPSd4JfIrmUMVPAn9EExg+\nVkq5N8kHgHcl2QrcB1wIXFdK+XK7ic/QhIRLkpwBPBY4D7iolPLDfhQkSZLmV+1hi58CPgr8OPB9\n4AvA2lLKXe3yUWAHcDlwCHAVcNr0yqWUnUlOBt5DMxuxDfgQcHbvJUiSpIVUe8LkrGcmllIeAE5v\nX7vrcwdwcs1+JUnS4uFtoSVJUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklTF8CBJ\nkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJ\nqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklRlTuEhyVuS7Ezyro62Q5JcnGQyyX1JLk+yomu9\nxye5Ism2JJuTnJ/EICNJ0hLQ8wd2kmcAvwN8tWvRBcBJwEuB44HHAZ/oWO8A4ErgIGAtcArwKuDc\nXsciSZIWTk/hIcljgEuB1wD3dLQvA04FRkspny+l3AS8GnhWkmPbbicCTwJeUUq5uZSyETgLOC3J\nQb2XIkmSFkKvMw8XA58qpXyuq/3pNDMK10w3lFJuAyaA49qmtcDNpZTJjvU2AkPAk3scjyRJWiDV\nv+kneRnwSzRBodtRwIOllHu72rcAK9s/r2zfdy+fXtZ9GESSJC0iVeEhyU/RnNPw/FLKD2tWBcpe\n9NubPtK8mJiYYHJycs8dB2x8fHzQQ5C0n6udeRgGfgK4MUnatgOB45O8HngBcEiSZV2zDyv40ezC\nZuAZXds9qv3aPSOxi9HRUYaGhnZpGxkZYWRkpLIMaVcTExMcc8xqtm+/f9BDkaSejY2NMTY2tkvb\n1NRU3/dTGx6uBp7S1fYhYBx4O/Ad4IfAOuBvAZIcDawCvtj2vx54a5LlHec9nABMAbfMtvMNGzaw\nZs2ayiFLezY5OdkGh0uB1YMezh5cSXOOsSTtaqZfqDdt2sTw8HBf91MVHkop2+j6gE+yDbirlDLe\nvv8A8K4kW4H7gAuB60opX25X+Uy7jUuSnAE8FjgPuKjyUIg0D1YDiz2gethC0mD149LI7vMURoEd\nwOXAIcBVwGkPdy5lZ5KTgffQzEZso5m9OLsPY5EkSfNszuGhlPKrXe8fAE5vX7tb5w7g5LnuW5Ik\nLTxvCS1JkqoYHiRJUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklTF8CBJkqoYHiRJ\nUhXDgyRJqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJqmJ4kCRJ\nVQwPkiSpiuFBkiRVMTxIkqQqhgdJklSlKjwk+d0kX00y1b6+mOQFHcsPSXJxkskk9yW5PMmKrm08\nPskVSbYl2Zzk/CSGGEmSlojaD+07gDOA4fb1OeCTSVa3yy8ATgJeChwPPA74xPTKbUi4EjgIWAuc\nArwKOLfnCiRJ0oI6qKZzKeWKrqYzk7wOWJvkO8CpwMtKKZ8HSPJqYDzJsaWULwEnAk8CnltKmQRu\nTnIW8PYk55RSHpprQZIkaX71fLggyQFJXgYcBlxPMxNxEHDNdJ9Sym3ABHBc27QWuLkNDtM2AkPA\nk3sdiyRJWjjV4SHJLya5D3gA+AvgJaWUW4GVwIOllHu7VtnSLqP9umWG5XT0kSRJi1jVYYvWrcBT\ngSNozm34cJLjZ+kfoOzFdvfYZ3R0lKGhoV3aRkZGGBkZ2YvNS5K0bxsbG2NsbGyXtqmpqb7vpzo8\ntOclfKN9uynJscAbgMuAg5Ms65p9WMGPZhc2A8/o2uRR7dfuGYlH2LBhA2vWrKkdsiRJ+4WZfqHe\ntGkTw8PDfd1PPy6RPAA4BLgReAhYN70gydHAKuCLbdP1wFOSLO9Y/wRgCrilD2ORJEnzrGrmIckf\nA5+muWTzx4BXAL8CnFBKuTfJB4B3JdkK3AdcCFxXSvlyu4nP0ISES5KcATwWOA+4qJTyw34UJEmS\n5lftYYujgA/TfOhPAV+jCQ6fa5ePAjuAy2lmI64CTpteuZSyM8nJwHtoZiO2AR8Czu69BEmStJBq\n7/Pwmj0sfwA4vX3trs8dwMk1+5UkSYuHt4WWJElVDA+SJKmK4UGSJFUxPEiSpCqGB0mSVMXwIEmS\nqhgeJElSlV4ejCVJ+7Tx8fFBD2GvLF++nFWrVg16GNoPGR4k6WHfBQ5g/fr1gx7IXjn00MO47bZx\nA4QWnOFBkh52D7ATuBRYPeCx7Mk427evZ3Jy0vCgBWd4kKRHWA2sGfQgpEXLEyYlSVIVw4MkSapi\neJAkSVUMD5IkqYrhQZIkVTE8SJKkKoYHSZJUxfAgSZKqGB4kSVIVw4MkSapieJAkSVUMD5IkqUpV\neEjyliRfSnJvki1J/jbJ0V19DklycZLJJPcluTzJiq4+j09yRZJtSTYnOT+JQUaSpCWg9gP7OcC7\ngWcCzwMeBXwmyaM7+lwAnAS8FDgeeBzwiemFbUi4kuaJnmuBU4BXAef2VIEkSVpQVY/kLqX8Wuf7\nJK8CvgcMA19Isgw4FXhZKeXzbZ9XA+NJji2lfAk4EXgS8NxSyiRwc5KzgLcnOaeU8tBci5IkSfNn\nrocKjgAKcHf7fpgmkFwz3aGUchswARzXNq0Fbm6Dw7SNwBDw5DmOR5IkzbOew0OS0Byi+EIp5Za2\neSXwYCnl3q7uW9pl0322zLCcjj6SJGmRqjps0eUvgF8Anr0XfUMzQ7Ene9NHkiQNUE/hIclFwK8B\nzyml3NmxaDNwcJJlXbMPK/jR7MJm4Bldmzyq/do9I7GL0dFRhoaGdmkbGRlhZGSksgJJkvY9Y2Nj\njI2N7dI2NTXV9/1Uh4c2OPxn4FdKKRNdi28EHgLWAX/b9j8aWAV8se1zPfDWJMs7zns4AZgCbmEW\nGzZsYM2aNbVDliRpvzDTL9SbNm1ieHi4r/upCg9J/gIYAV4EbEsyPWMwVUrZXkq5N8kHgHcl2Qrc\nB1wIXFdK+XLb9zM0IeGSJGcAjwXOAy4qpfxw7iVJkqT5VDvz8Ls05yX83672VwMfbv88CuwALgcO\nAa4CTpvuWErZmeRk4D00sxHbgA8BZ1eORZIkDUDtfR72eHVGKeUB4PT2tbs+dwAn1+xbkiQtDt4S\nWpIkVTE8SJKkKoYHSZJUxfAgSZKqGB4kSVIVw4MkSapieJAkSVUMD5IkqYrhQZIkVTE8SJKkKoYH\nSZJUxfAgSZKqGB4kSVIVw4MkSapieJAkSVUMD5IkqYrhQZIkVTE8SJKkKoYHSZJUxfAgSZKqGB4k\nSVIVw4MkSapieJAkSVUMD5IkqUp1eEjynCR/n+Q7SXYmedEMfc5NcmeS+5N8NskTu5YfmeQjSaaS\nbE3y/iSHz6UQSZK0MHqZeTgc+GfgNKB0L0xyBvB64LXAscA2YGOSgzu6fRRYDawDTgKOB97bw1gk\nSdICO6h2hVLKVcBVAEkyQ5c3AOeVUj7V9nklsAV4MXBZktXAicBwKeWmts/pwBVJ3lhK2dxTJZIk\naUH09ZyHJE8AVgLXTLeVUu4FbgCOa5vWAlung0PrappZjGf2czySJKn/+n3C5EqaELClq31Lu2y6\nz/c6F5ZSdgB3d/SRJEmLVPVhix6FGc6PqO0zOjrK0NDQLm0jIyOMjIzMbXSSJO0DxsbGGBsb26Vt\namqq7/vpd3jYTBMCjmLX2YcVwE0dfVZ0rpTkQOBIHjljsYsNGzawZs2avg1WkqR9yUy/UG/atInh\n4eG+7qev4aGUcnuSzTRXUXwNIMkymnMZLm67XQ8ckeRpHec9rKMJHTf0czwavImJCSYnJwc9jD0a\nHx8f9BAkacmoDg/t/RieSPNhD/CzSZ4K3F1KuQO4ADgzydeBbwLnAd8GPglQSrk1yUbgfUleBxwM\nvBsY80qLfcvExATHHLOa7dvvH/RQJEl91MvMw9OBf6A5P6EAf9a2/zVwainl/CSH0dy34QjgWuCF\npZQHO7bxcuAimqssdgKX01ziqX3I5ORkGxwupbmtx2J2JXDWoAchSUtCL/d5+Dx7uEqjlHIOcM4s\ny+8B1tfuW0vVamCxn6viYQtJ2ls+20KSJFUxPEiSpCqGB0mSVMXwIEmSqhgeJElSFcODJEmqYniQ\nJElVDA+SJKmK4UGSJFVZqEdyS5LmwVJ6qNvy5ctZtWrVoIehPjA8SNKS9F3gANavXzp3+j/00MO4\n7bZxA8Q+wPAgSUvSPTTPFVwKD54DGGf79vVMTk4aHvYBhgdJWtKWwoPntK/xhElJklTF8CBJkqoY\nHiRJUhXPeVhiJiYmmJycHPQw9spSuoRMkrT3DA9LyMTEBMccs5rt2+8f9FAkSfsxw8MSMjk52QaH\npXJp1pXAWYMehCSpzwwPS9JSuTTLwxaStC/yhElJklTF8CBJkqp42GKRGRsbY2RkZNDDWABjgHXu\nO/aXOmH/qXV+6lxsV2FdddVVvOAFL3hEuw/xmt1Aw0OS04A3AiuBrwKnl1K+PMgxDZrhYV9jnfue\n/aXWfte5eB/k9ba3ve0RbT7Ea3YDCw9Jfhv4M+C/AV8CRoGNSY4upSyNGxlIkvbSYn2Q1yiwoavN\nh3jtySBnHkaB95ZSPgyQ5HeBk4BTgfMHOC5J0rxZbFeLDbG78Sy2Qyy7M4hDLAMJD0keBQwDfzLd\nVkopSa4GjlvIsdxyyy284x3vWMhdzuqmm27ilFNOmXHZXXfdtcCjkaT90eI9xDKTQRxiGdTMw3Lg\nQGBLV/sW4JgZ+h8K85MCzzzzTD796U/3fbtz8eEPf3gPPa5kadxD4br260zj/TbwkYUdzqxmG+tc\nzEed8zXWuZitzsU43t3Zm7Eulp/d+f577Xedi/XnYKY6r6M5xPJfgccu+IjqfJft2z/Atddey+rV\nMx8O6vjsPLRfe00ppV/b2vudJo8FvgMcV0q5oaP9fODZpZRf7ur/chbHv1ZJkpaqV5RSPtqPDQ1q\n5mES2AEc1dW+gkfORgBsBF4BfBPYPq8jkyRp33Io8DM0n6V9MZCZB4Ak/wTcUEp5Q/s+wARwYSnl\nnQMZlCRJ2qNBXm3xLuCvk9zIjy7VPAz40ADHJEmS9mBg4aGUclmS5cC5NIcv/hk4sZTy/UGNSZIk\n7dnADltIkqSlyQdjSZKkKoYHSZJUZdGGhyRHJvlIkqkkW5O8P8nhFet/OsnOJC+az3HOVS91JvnL\nJF9Pcn+S7yX5uyQz3Vxr0aits+1/YZJbk2xL8q0kf55k2UKOu1aP38/fSfIP7To7F2ONSU5LcnuS\nHyT5pyTP2EP//5JkvO3/1SQvXKixzkVNnUl+Icnlbf+dSX5/Icc6V5W1vibJPya5u319dk8/A4tF\nZZ0vSfLl9t/ufyS5KcmSuM1k7b/RjvVe1v78/k3N/hZteAA+SnMT9HU0z7w4Hnjv3qyYZJTmPhJL\n4YSOXur8CvAq4EnACUBoHiqW+RvmnNXW+TiaW7v9AfCLwCnAC4D3z+8w56yX7+ejgU8Df8wi/Jnt\neIjd2cDTaJ6Au7E94Xmm/sfR/D28D/gl4O+Av0vyCwsz4t7U1klzddi/A2fQ3M94yeih1l+h+Z7+\nJ2AtcAfwmfaGf4tWD3XeBfxPmhqfAnwQ+GCS5y/AcHvWQ53T6/008E7gH6t3WkpZdC+aD8WdwNM6\n2k4EHgJW7mHdpwLfornh1E7gRYOuZz7q7NrOU2jC0hMGXdM81/mbwA+AAwZd03zUSfMf9A5g2aBr\n6RrXPwF/3vE+NPf0fdNu+n8M+PuutuuBvxh0Lf2ss2vd24HfH3QNC1Fr2/8AYApYP+ha5rPOdp0b\ngT8adC39rrP9Hl4LvJomJP1NzT4X68zDccDWUspNHW1X0/xW9szdrZTk0TTp+LRSyvfmd4h90VOd\nndop8VOBb9D8NrAYzbnO1hHAvaWUnf0cXB/1q85Fo+MhdtdMt5Xmf57ZHmJ3XLu808ZZ+g9cj3Uu\nSX2q9XCq5NlAAAAEFUlEQVTgUcDdfR9gn/SjziTrgKOBz8/HGPthDnWeDXyvlPLBXva7WMPDSmCX\nD/9Syg6aH9SVs6y3AfhCKeX/zOPY+qnXOknyuiT3AffRHLo4oZTy0HwNdI56rnNaO/12Jnt56GpA\n5lznIjTbQ+x2V9PKyv6LQS91LlX9qPUdNM8n6g6Ji0lPdSZZluS+JA8CnwJOL6V8bv6GOWfVdSZ5\nFs2Mw2t63emChockf9qemLG7144kR8+2CXZzTDjNiZG/SnOnyoGazzo7XEpzPPl44N+Ajyc5uE8l\n7JUFqpMkPwZcAfwL8Ed9Gv5eW6g6l5jampbq38FSHXcv9vbf45uB3wJeXEp5cN5H1X97qvM+msPf\nTwfeBmxIcvxCDKzPZqwzyWOAS4DfKaVs7XXjC32Hyf9Fc2xlNt8ANtOcs/CwJAcCRzLzg7MAngv8\nLDDVdd7g3yT5x1LKr/Y04t7MZ50AlFKmZx3+PckNwFbgJcD/7nHMvZj3Otsf9I3APcBvtL/JL7R5\nr3MRq32IHTR/DzX9F4Ne6lyqeq41yRuBNwHrSin/Oj/D65ue6myn/L/Rvv1ae6LvW+jlpMKFUVvn\nzwE/DXyq4yT7AwDa2ZZjSim372mnCxoeSil30ZzNOqsk1wNHJHlax/HjdTRJ6obdrPanNGd3d/oX\n4A3Agh7GmOc6Z3JAu84htWOdi/mus51x2EhzkuSLBvVbzgC+n4tGKeWHaZ4/sw74e3j4IXbrgAt3\ns9r1Myx/ftu+KPVY55LUa61J/jvwVppDpDftrt9i0cfv6QEs8P+tNXqoc5zmJPtOfww8Bvh99vbc\nuUGfJTrLmaBX0lyS+AzgWcBtwCUdyx/X/iU8fZZtLOqrLXqpE3gC8GZgDfB44JfbH5jvA8sHXU8f\n63wMzRnE/9zWfFTHa1FebdFLnW3bUTTTpK9pf2af3b4/ctD1tOP7LZoA90qaK0reSxOmfqJd/mHg\nTzr6Hwc8SHOZ7THAOcB24BcGXUuf63xU+336JZrj/+9o3//coGuZh1rf1H4PX9L1b/HwQdfS5zrf\nDDyv/T/nScAfAg8Arx50Lf2sc4b1q6+2GHjRsxRzBM1x/SmaKfn3AYd1LP9pmqma42fZxg4Wf3io\nqpPm3gdX0FxXvp3mstRLgJ8fdC19rnP6ssXO187266pB19OvOtu2sztq63y9ctD1dIzx94Bvtv9B\nXc+u4edzwF919X8pcGvb/2s0D70beB39rLP9Xs70ffvcoOuYh1pvn6HOHcD/GHQdfa7zPJrAv43m\ncMAXgN8cdA39rnOGdavDgw/GkiRJVRbrpZqSJGmRMjxIkqQqhgdJklTF8CBJkqoYHiRJUhXDgyRJ\nqmJ4kCRJVQwPkiSpiuFBkiRVMTxIkqQqhgdJklTl/wPH5JyZhO6S8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70853d55d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of non zero elements in original data are: ', 2400)\n",
      "the indices of the largest 10 weights and their values are\n",
      "(8, 0, 2, 2) 0.385424\n",
      "(3, 1, 2, 2) -0.360212\n",
      "(11, 0, 1, 2) 0.33931\n",
      "(8, 2, 2, 2) 0.338287\n",
      "(31, 1, 1, 2) -0.334987\n",
      "(8, 1, 2, 2) 0.331826\n",
      "(3, 0, 2, 2) -0.32304\n",
      "(11, 2, 1, 2) 0.32127\n",
      "(31, 1, 2, 1) 0.320731\n",
      "(3, 2, 2, 2) -0.308671\n"
     ]
    }
   ],
   "source": [
    "def layer_stats(solver,layername):\n",
    "    print('stats for layer',layername)\n",
    "    original_data=np.copy(solver.net.params[layername][0].data[:])\n",
    "    absolute_orig_data=np.abs(original_data)\n",
    "    print('max, min and mean value  of original data are ',np.max(original_data),np.min(original_data),np.mean(original_data))\n",
    "    print('max, min and mean value  of absolute value of original data  are ',np.max(absolute_orig_data),np.min(absolute_orig_data),np.mean(absolute_orig_data))\n",
    "    plt.hist(original_data.flatten(),bins=10)\n",
    "    plt.show()\n",
    "    \n",
    "    ## Sort the weigh indices according to magnitude, in descending order\n",
    "    train_abs=np.copy(absolute_orig_data)\n",
    "    print('number of non zero elements in original data are: ',np.count_nonzero(train_abs))\n",
    "\n",
    "    flat_sort_rev_trainwt=[]\n",
    "\n",
    "    #code to rank weights\n",
    "    for index, x in np.ndenumerate(train_abs):\n",
    "      flat_sort_rev_trainwt.append([index,x])\n",
    "\n",
    "\n",
    "    flat_sort_rev_trainwt.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    print('the indices of the largest 10 weights and their values are')\n",
    "    trainwt_bottom=[]\n",
    "    for i in range(10): \n",
    "      trainwt_bottom.append(flat_sort_rev_trainwt[i][0][:])\n",
    "      print trainwt_bottom[-1],original_data[trainwt_bottom[-1]]\n",
    "    \n",
    "    return flat_sort_rev_trainwt\n",
    "\n",
    "flat_sort_rev_trainwt= layer_stats(solver,'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUNING SETUP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a mask to mask out the weights that you want to prune out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of elements in the layer = 2400\n",
      "number of weights retained = 100\n",
      "non zero elements and sum of mask=100.0\n",
      "zeroed out elements=2300.0\n"
     ]
    }
   ],
   "source": [
    "def create_mask(solver,layername,num_elements_retained,flat_sort_rev_trainwt):\n",
    "    mask=np.zeros(np.shape(solver.net.params[layername][0].data[:]))\n",
    "    total=len(flat_sort_rev_trainwt)\n",
    "\n",
    "    for i in range(num_elements_retained):\n",
    "          mask[flat_sort_rev_trainwt[i][0]]=1\n",
    "\n",
    "\n",
    "    print 'total number of elements in the layer = ' + str(total)\n",
    "    print 'number of weights retained = ' + str(num_elements_retained)\n",
    "    print 'non zero elements and sum of mask='+str(sum(mask))\n",
    "    print 'zeroed out elements=' + str(total-sum(mask))\n",
    "    \n",
    "    return mask\n",
    "    \n",
    "mask=create_mask(solver,'conv1',100,flat_sort_rev_trainwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Create the training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training loop\n",
    "def training_loop_pruning(solver,mask,layername,niter=60000):\n",
    "  print('starting training')\n",
    " # niter = 60000\n",
    "  test_interval = 1000\n",
    "  train_loss = zeros(niter)\n",
    "  test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "  output = zeros((niter, 8, 10))\n",
    "\n",
    "# the main solver loop\n",
    "  for it in range(niter):\n",
    "      solver.step(1)  # SGD by Caffe\n",
    "    \n",
    "    # store the train loss\n",
    "      train_loss[it] = solver.net.blobs['loss'].data\n",
    "    \n",
    "    # store the output on the first test batch\n",
    "    # (start the forward pass at ip1 to avoid loading new data)\n",
    "      solver.test_nets[0].forward(start='conv1')\n",
    "      output[it] = solver.test_nets[0].blobs['ip1'].data[:8]\n",
    "      solver.net.params[layername][0].data[:] = solver.net.params[layername][0].data[:] * mask\n",
    "        \n",
    "    # run a full test every so often\n",
    "    # (Caffe can also do this for us and write to a log, but we show here\n",
    "    #  how to do it directly in Python, where more complicated things are easier.)\n",
    "      if it % test_interval == 0:\n",
    "       # print 'Iteration', it, 'testing...'\n",
    "       # print('nonzero in ip1 num are:',len(np.nonzero(solver.test_nets[0].params['ip1'][0].data[:])[0]))\n",
    "        correct = 0\n",
    "        for test_it in range(100):\n",
    "            solver.test_nets[0].forward()\n",
    "            correct += sum(solver.test_nets[0].blobs['ip1'].data.argmax(1)\n",
    "                           == solver.test_nets[0].blobs['label'].data)\n",
    "        test_acc[it // test_interval] = correct / 1e4\n",
    "  return test_acc[-1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_with_mask(elements_to_retain,flat_sort_rev_trainwt,solver,layername,niter=60000):\n",
    "  solver.net.copy_from('/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/reference.caffemodel.h5')\n",
    "  print('solver loaded')\n",
    "\n",
    "\n",
    "  mask=np.zeros(np.shape(solver.net.params[layername][0].data[:]))\n",
    "  total=len(flat_sort_rev_trainwt)\n",
    "  for i in range(elements_to_retain):\n",
    "    mask[flat_sort_rev_trainwt[i][0]]=1\n",
    "\n",
    "\n",
    "  print 'total number of elements = '  + str(total)\n",
    "  print 'thresh_percent, number of weights retained = ' + str(elements_to_retain)\n",
    "  print 'non zero elements and sum of mask='+str(sum(mask))\n",
    "  print 'zeroed out elements=' + str(total-sum(mask))\n",
    "\n",
    " \n",
    "  solver.net.params[layername][0].data[:] = solver.net.params[layername][0].data[:] * mask\n",
    "  print('starting training for bot_thresh=',elements_to_retain)\n",
    "  test_acc=training_loop_pruning(solver,mask,layername,niter)\n",
    "  print('testing accuracy is: ',test_acc)\n",
    "  return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8183\n"
     ]
    }
   ],
   "source": [
    " \n",
    "solver.test_nets[0].copy_from('/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/reference.caffemodel.h5')\n",
    "#solver.test_nets[0].params['conv1'][0].data[:]=np.copy(new100)\n",
    "\n",
    "correct = 0\n",
    "for test_it in range(100):\n",
    "    solver.test_nets[0].forward()\n",
    "    correct += sum(solver.test_nets[0].blobs['ip1'].data.argmax(1)\n",
    "                           == solver.test_nets[0].blobs['label'].data)\n",
    "print(correct/1e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decide pruning thresholds (number of weights you want to retain), and retrain keeping only those elemts active via a mask. Decide upon the final threshold, and save the trained model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('running case', 10)\n",
      "solver loaded\n",
      "total number of elements = 2400\n",
      "thresh_percent, number of weights retained = 10\n",
      "non zero elements and sum of mask=10.0\n",
      "zeroed out elements=2390.0\n",
      "('starting training for bot_thresh=', 10)\n",
      "starting training\n"
     ]
    }
   ],
   "source": [
    "accuracy=[]\n",
    "retained=[10,20,40,60,80,100]\n",
    "for retain in retained:\n",
    "    print('running case',retain)\n",
    "    accuracy.append(train_with_mask(retain,flat_sort_rev_trainwt,solver,'conv1',niter=60000))\n",
    "    \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of nonzero weights are ', 1350)\n",
      "('accuracy is ', 0.73899999999999999)\n"
     ]
    }
   ],
   "source": [
    "#retest pruned solver after loading the relevant model.\n",
    "solver.net.copy_from('/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/reference100.caffemodel.h5')\n",
    "solver.test_nets[0].copy_from('/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/reference100.caffemodel.h5')\n",
    "original_data100=np.copy(solver.net.params['conv1'][0].data[:])\n",
    "\n",
    "print('number of nonzero weights are ',len(np.nonzero(solver.net.params['conv1'][0].data[:])[0]))\n",
    "\n",
    "correct = 0\n",
    "for test_it in range(100):\n",
    "    solver.test_nets[0].forward()\n",
    "    correct += sum(solver.test_nets[0].blobs['ip1'].data.argmax(1)\n",
    "                           == solver.test_nets[0].blobs['label'].data)\n",
    "print('accuracy is ',correct/1e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUANTIZATION SETUP\n",
    "\n",
    "\n",
    "#### Save the pruned  layer weights and create the mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_data100=np.copy(solver.net.params['conv1'][0].data[:])\n",
    "mask=np.zeros(np.shape(solver.net.params['conv1'][0].data[:]))\n",
    "for i in range(100):\n",
    "  mask[flat_sort_rev_trainwt[i][0]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# quantizer weights according to codebok and code\n",
    "def quantize_weights(solver,codebook,code,layername):\n",
    "    original_data=np.copy(solver.net.params[layername][0].data[:])\n",
    "    weight_conv1_flattened=original_data.flatten()\n",
    "    nz_idx=np.nonzero(weight_conv1_flattened)\n",
    "    conv1_flat_changed=np.zeros(weight_conv1_flattened.shape)\n",
    "    for idx,nzidx in enumerate(nz_idx[0]):\n",
    "        conv1_flat_changed[nzidx]=codebook[code[idx]]\n",
    "    conv1_changed=conv1_flat_changed.reshape(original_data.shape)\n",
    "    return conv1_changed\n",
    "\n",
    "\n",
    "#quantize the backward gradients\n",
    "def quantize_grads(solver,code,codebook,layername):\n",
    "    original_data=np.copy(solver.net.params[layername][0].data[:])\n",
    "    weight_conv1_flattened=original_data.flatten()\n",
    "    nz_idx=np.nonzero(weight_conv1_flattened)\n",
    "    gradients=solver.net.params[layername][0].diff\n",
    "    gradients_codebook=np.zeros((codebook.shape))\n",
    "    grads_flatten=gradients.flatten()\n",
    "    final_grads=[]\n",
    "    [final_grads.append(grads_flatten[nzidx]) for nzidx in nz_idx[0]]\n",
    "    for idx,grad in enumerate(final_grads):\n",
    "        gradients_codebook[code[idx]]+=grad\n",
    "    return gradients_codebook\n",
    "\n",
    "\n",
    "#update the codebook with the cumulative gradient, sort of like a custom solver.step(1) for the codebook\n",
    "def update_codebook(codebook,gradients_codebook,learning_rate,velocity,momentum):\n",
    "    velocity=np.multiply(momentum,velocity)-np.multiply(learning_rate,gradients_codebook)\n",
    "    codebook+=velocity\n",
    "    return codebook,velocity\n",
    "        \n",
    "#wrap forward, backward and update step into 1 training step       \n",
    "def quantized_training_step(solver,codebook,code,mask,learning_rate,velocity,momentum,layername):\n",
    "    solver.step(1)\n",
    "    solver.net.params[layername][0].data[:]= solver.net.params[layername][0].data[:]*mask\n",
    "    gradients_codebook=quantize_grads(solver,code,codebook,'conv1')\n",
    "    codebook,velocity=update_codebook(codebook,gradients_codebook,learning_rate,velocity,momentum)\n",
    "    quantized_weights=quantize_weights(solver,codebook,code,'conv1')\n",
    "    solver.net.params[layername][0].data[:]=np.copy(quantized_weights)\n",
    "    for k, v in solver.net.params.items():\n",
    "        solver.test_nets[0].params[k][0].data[:]= np.copy(v[0].data)\n",
    "\n",
    "    \n",
    "#generate codebook from kmeans clustering trained weights\n",
    "#using uniform cenroid initialization as that gives best results\n",
    "def cluster_trained_weights(solver,numClusters,layername):\n",
    "    original_data=np.copy(solver.net.params[layername][0].data[:])\n",
    "    nz_idx=np.nonzero(original_data)\n",
    "    nz_data=original_data[nz_idx]\n",
    "    X=nz_data.flatten()\n",
    "    X=X.reshape(-1,1)\n",
    "    #X=whiten(X)\n",
    "    InitC=np.linspace(X.min(),X.max(),num=numClusters)\n",
    "    codebook, codeX=kmeans2(X, InitC.reshape(-1,1), minit='matrix')\n",
    "    #codebook, codeX=kmeans2(X, numClusters, minit='points')\n",
    "    #codeX,dist=vq(X,codebook)\n",
    "    #print(code.min(),code.max())\n",
    "    edges_hist=[x for x in range(numClusters+1)]\n",
    "    frq, edges = np.histogram(codeX,edges_hist)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(edges[:-1], frq, width=np.diff(edges), ec=\"k\", align=\"edge\")\n",
    "    plt.show()\n",
    "    return codeX,codebook\n",
    "\n",
    "\n",
    "\n",
    "#training loop\n",
    "def training_loop(solver,niter,codebook,code,mask,learning_rate,velocity,momentum,layername):\n",
    "  print('starting training')\n",
    "  test_interval=1000\n",
    "  train_loss = zeros(niter)\n",
    "  test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "  output = zeros((niter, 8, 10))\n",
    "\n",
    "\n",
    "# the main solver loop\n",
    "  for it in range(niter):\n",
    "    #if it>niter/2: learning_rate=learning_rate/8 #uncomment this if you need to reduce learning rate\n",
    "            \n",
    "    # solver.step(1)\n",
    "      quantized_training_step(solver,codebook,code,mask,learning_rate,velocity,momentum,'conv1')  # custom SGD step for quantization\n",
    "    \n",
    "    # store the train loss\n",
    "      train_loss[it] = solver.net.blobs['loss'].data\n",
    "    # (start the forward pass at conv1 to avoid loading new data)\n",
    "      solver.test_nets[0].forward(start='conv1')\n",
    "      output[it] = solver.test_nets[0].blobs['ip1'].data[:8]\n",
    "      solver.net.params[layername][0].data[:] = solver.net.params[layername][0].data[:]  * mask\n",
    "    # run a full test every so often\n",
    "      if it % test_interval == 0:\n",
    "       # print 'Iteration', it, 'testing...'\n",
    "        correct = 0\n",
    "        for test_it in range(100):\n",
    "            solver.test_nets[0].forward()\n",
    "            correct += sum(solver.test_nets[0].blobs['ip1'].data.argmax(1)\n",
    "                           == solver.test_nets[0].blobs['label'].data)\n",
    "        test_acc[it // test_interval] = correct / 1e4\n",
    "        print('accuracy for this iteration is',correct/1e4)\n",
    "    \n",
    "  _, ax1 = subplots()\n",
    "  ax2 = ax1.twinx()\n",
    "  ax1.plot(arange(niter), train_loss)\n",
    "  ax2.plot(test_interval * arange(len(test_acc)), test_acc, 'r')\n",
    "  ax1.set_xlabel('iteration')\n",
    "  ax1.set_ylabel('train loss')\n",
    "  ax2.set_ylabel('test accuracy')\n",
    "  ax2.set_title('Test Accuracy: {:.2f}'.format(test_acc[-1]))\n",
    "        \n",
    "  return test_acc[-1]\n",
    "\n",
    "\n",
    "def refresh_solver(solver,data,reference,layername):\n",
    "    solver.net.copy_from(reference)\n",
    "    solver.net.params[layername][0].data[:]=np.copy(data)\n",
    "\n",
    "def top_level_training(solver,clusters,mask,data,reference,layername,learning_rate=0.0001,momentum=0.9,niter=60000):\n",
    "    refresh_solver(solver,data,reference,'conv1')\n",
    "    print('toplevel shape nonzero is',len(np.nonzero(solver.net.params[layername][0].data[:])[0]))\n",
    "    code,codebook = cluster_trained_weights(solver,clusters,layername)\n",
    "    print('shape of code and codebook is ',code.shape,codebook)\n",
    "    solver.net.params[layername][0].data[:]=np.copy(quantize_weights(solver,codebook,code,layername))\n",
    "    print('toplevel shape nonzero is',len(np.nonzero(solver.net.params[layername][0].data[:])[0]))\n",
    "    velocity=np.zeros(codebook.shape) #TODO: possible bug. Check how velocity is initialized\n",
    "    quantized_training_step(solver,codebook,code,mask,learning_rate,velocity,momentum,layername)\n",
    "    return training_loop(solver,niter,codebook,code,mask,learning_rate,velocity,momentum,layername)\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "data100=original_data100\n",
    "reference100='/home/super/ISHA/Deep_Learning/16Nov/neuron_wise_exhaustive/reference100.caffemodel.h5'\n",
    "mask100=np.zeros(np.shape(solver.net.params['conv1'][0].data[:]))\n",
    "for i in range(100):\n",
    "  mask100[flat_sort_rev_trainwt[i][0]]=1\n",
    "    \n",
    "print(len(np.nonzero(original_data100)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('running for clusters', 2)\n",
      "('toplevel shape nonzero is', 1350)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAFkCAYAAABijEI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVJzBgxJktScAUOSJDX3/wFUwsH63Lm7/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f700c398250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shape of code and codebook is ', (1350,), array([[-0.43558228],\n",
      "       [ 0.01160863]], dtype=float32))\n",
      "('toplevel shape nonzero is', 1350)\n",
      "starting training\n",
      "('accuracy for this iteration is', 0.1145)\n",
      "('accuracy for this iteration is', 0.41339999999999999)\n",
      "('accuracy for this iteration is', 0.44979999999999998)\n",
      "('accuracy for this iteration is', 0.47420000000000001)\n",
      "('accuracy for this iteration is', 0.49419999999999997)\n",
      "('accuracy for this iteration is', 0.5121)\n",
      "('accuracy for this iteration is', 0.52400000000000002)\n",
      "('accuracy for this iteration is', 0.53300000000000003)\n",
      "('accuracy for this iteration is', 0.5423)\n",
      "('accuracy for this iteration is', 0.55179999999999996)\n",
      "('accuracy for this iteration is', 0.55769999999999997)\n",
      "('accuracy for this iteration is', 0.5605)\n",
      "('accuracy for this iteration is', 0.56520000000000004)\n",
      "('accuracy for this iteration is', 0.57040000000000002)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-93904cb7bbb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumClusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running for clusters'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfinal_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_level_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bebac18ec989>\u001b[0m in \u001b[0;36mtop_level_training\u001b[0;34m(solver, clusters, mask, data, reference, layername, learning_rate, momentum, niter)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mvelocity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: possible bug. Check how velocity is initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mquantized_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodebook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvelocity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodebook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvelocity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bebac18ec989>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(solver, niter, codebook, code, mask, learning_rate, velocity, momentum, layername)\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# (start the forward pass at conv1 to avoid loading new data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_nets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ip1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayername\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayername\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/super/ISHA/Deep_Learning/Caffe/caffe/python/caffe/pycaffe.pyc\u001b[0m in \u001b[0;36m_Net_forward\u001b[0;34m(self, blobs, start, end, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# Unpack blobs to extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.cluster.vq import vq, kmeans, whiten, kmeans2\n",
    "\n",
    "numClusters=[2,4,8,16,32]\n",
    "final_acc=[]\n",
    "for nc in numClusters:\n",
    "    print('running for clusters',nc)\n",
    "    final_acc.append(top_level_training(solver,nc,mask100,data100,reference100,'conv1',niter=60000))\n",
    "    print('final_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.nonzero(solver.net.params['conv1'][0].data[:])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
